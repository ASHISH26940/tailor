# ğŸš¢ Titanic Dataset Chat Agent

An AI-powered chatbot that lets you explore the Titanic dataset through natural language. Ask questions in plain English and get text answers, statistics, and auto-generated visualizations.

![Python](https://img.shields.io/badge/Python-3.10+-3776AB?logo=python&logoColor=white)
![FastAPI](https://img.shields.io/badge/FastAPI-009688?logo=fastapi&logoColor=white)
![Streamlit](https://img.shields.io/badge/Streamlit-FF4B4B?logo=streamlit&logoColor=white)
![LangChain](https://img.shields.io/badge/LangChain-1C3C3C?logo=langchain&logoColor=white)

## Features

- **Natural language queries** â€” ask anything about the Titanic dataset
- **Auto-generated charts** â€” request histograms, bar charts, scatter plots, etc.
- **Conversational UI** â€” chat interface with full message history
- **Real-time analysis** â€” powered by a LangChain ReAct agent that writes and executes Python code on the fly
- **Redis caching** â€” repeated questions return instantly from cache (1-hour TTL, graceful fallback if Redis unavailable)

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Streamlit (8501)  â”‚  POST   â”‚       FastAPI (8000)         â”‚
â”‚                     â”‚ â”€â”€â”€â”€â”€â”€â–º â”‚                              â”‚
â”‚  â€¢ Chat UI          â”‚ /chat   â”‚  â€¢ Redis cache check         â”‚
â”‚  â€¢ Message history  â”‚ â—„â”€â”€â”€â”€â”€â”€ â”‚  â€¢ LangChain ReAct Agent     â”‚
â”‚  â€¢ Image rendering  â”‚  JSON   â”‚  â€¢ Plot generation â†’ static/ â”‚
â”‚                     â”‚         â”‚  â€¢ OpenRouter LLM            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Quick Start

### 1. Clone & install dependencies

```bash
git clone <repo-url>
cd tailortalk
python -m venv venv
venv\Scripts\activate        # Windows
# source venv/bin/activate   # macOS/Linux
pip install -r requirements.txt
```

### 2. Configure environment

Copy the example env file and add your API key:

```bash
cp .example.env .env
```

Edit `.env` and set your OpenRouter API key:

```env
OPENAI_API_KEY=sk-or-v1-your-key-here
```

### 3. Start the backend

```bash
uvicorn backend.main:app --reload
```

The API will be available at `http://localhost:8000`.

### 4. Start the frontend

In a separate terminal:

```bash
streamlit run frontend/app.py
```

The UI will open at `http://localhost:8501`.

## Example Queries

| Query                                          | Expected Output                |
| ---------------------------------------------- | ------------------------------ |
| "What percentage of passengers were male?"     | Text answer with percentage    |
| "Show me a histogram of passenger ages"        | Age distribution chart         |
| "What was the average ticket fare?"            | Text answer with fare value    |
| "How many passengers embarked from each port?" | Count breakdown or bar chart   |
| "What was the survival rate by gender?"        | Comparison statistics or chart |

## Project Structure

```
tailortalk/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ agent.py          # LangChain agent setup & LLM config
â”‚   â””â”€â”€ main.py           # FastAPI app & /chat endpoint
â”œâ”€â”€ frontend/
â”‚   â””â”€â”€ app.py            # Streamlit chat interface
â”œâ”€â”€ titanic/
â”‚   â””â”€â”€ train.csv         # Titanic dataset
â”œâ”€â”€ static/               # Auto-generated plot images
â”œâ”€â”€ .streamlit/
â”‚   â””â”€â”€ config.toml       # Streamlit theme config
â”œâ”€â”€ .env                  # API keys (not committed)
â”œâ”€â”€ .example.env          # Example env template
â”œâ”€â”€ requirements.txt      # Python dependencies
â””â”€â”€ README.md
```

## Tech Stack

- **Backend**: [FastAPI](https://fastapi.tiangolo.com/) + [Uvicorn](https://www.uvicorn.org/)
- **AI Agent**: [LangChain](https://www.langchain.com/) ReAct Agent with `PythonAstREPLTool`
- **LLM**: `openai/gpt-oss-120b` via [OpenRouter](https://openrouter.ai/)
- **Caching**: [Redis](https://redis.io/) with graceful fallback
- **Visualization**: [Matplotlib](https://matplotlib.org/) + [Seaborn](https://seaborn.pydata.org/)
- **Frontend**: [Streamlit](https://streamlit.io/) with custom CSS theming

## How It Works

1. User types a question in the Streamlit chat interface
2. Streamlit sends a `POST` request to the FastAPI `/chat` endpoint
3. FastAPI checks Redis for a cached response â€” if found, returns it instantly
4. On cache miss, the LangChain ReAct agent interprets the question, writes Python/Pandas code, and executes it
5. If a visualization is requested, the agent saves the plot to `static/` with a unique filename
6. Text-only responses are cached in Redis (1-hour TTL); plot responses are not cached
7. FastAPI parses the agent's response, extracts any image paths, and returns a clean JSON response
8. Streamlit renders the text answer and any generated charts in the chat

## License

MIT
